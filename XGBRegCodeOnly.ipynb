{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Output of code ommited for clarity of script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn import decomposition\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "\n",
    "data = pd.read_csv('C:/Users/u0132612/Documents/PhD KULeuven/Lessen/Advanced Analytics in a Big Data World/Assignments/Assignment 2/train.csv')\n",
    "data_test = pd.read_csv('C:/Users/u0132612/Documents/PhD KULeuven/Lessen/Advanced Analytics in a Big Data World/Assignments/Assignment 2/test.csv')\n",
    "\n",
    "data.info()\n",
    "\n",
    "\n",
    "#SAME PREPROCESSING as done by Steven\n",
    "\n",
    "data.pixels_y = data.pixels_y.astype('int64')\n",
    "\n",
    "data_test.pixels_y = data_test.pixels_y.fillna(0)\n",
    "data_test.pixels_y = data_test.pixels_y.astype('int64')\n",
    "\n",
    "#selecteer enkel usefull information\n",
    "data_s = data[['id','brand', 'screen_size','pixels_y','screen_surface','touchscreen','cpu', \"cpu_details\", 'discrete_gpu','gpu','os','os_details','ram','ssd','storage','weight','min_price','max_price']]\n",
    "data_test_s = data_test[['id','brand', 'screen_size','pixels_y','screen_surface','touchscreen','cpu', 'cpu_details', 'discrete_gpu','gpu',\"os\",'os_details','ram','ssd','storage','weight']]\n",
    "\n",
    "#first basic reformatter\n",
    "def reformat(df):\n",
    "    df_pp = df.copy()\n",
    "    df_pp[\"cpu_brand\"] = df_pp.cpu.str.split(n=1).str[0]\n",
    "    df_pp[\"cpu_type\"] = df_pp.cpu.str.split(n=1).str[1]\n",
    "    df_pp[\"gpu_brand\"] = df_pp.gpu.str.split(n=1).str[0]\n",
    "    df_pp[\"gpu_series\"] = df_pp.gpu.str.split(n=2).str[1]\n",
    "    df_pp[\"os_type\"] = df_pp.os_details.str.split(n=1).str[1]\n",
    "    df_pp[\"cpu_gen\"] = df_pp['cpu_details'].str.split(\"(\").str[1].str.split(\")\").str[0].str.split(\" \",n=3).str[0]\n",
    "    df_pp = df_pp.drop(columns=[\"os_details\", \"cpu\", \"gpu\", \"os_details\"])\n",
    "    return df_pp\n",
    "\n",
    "#automatische onehot encoder\n",
    "def process_onehot(df, list_values):\n",
    "    transformed_df = pd.DataFrame()\n",
    "    from sklearn import preprocessing\n",
    "    for item in list_values:\n",
    "        enc = preprocessing.OneHotEncoder()\n",
    "        df[item] = df[item].astype(str)\n",
    "        df[item] = df[item].str.lower()\n",
    "        enc.fit(df[[item]].dropna())\n",
    "        new_df = pd.DataFrame(data=enc.transform(df[[item]].dropna()).toarray(), columns=enc.categories_[0])\n",
    "        transformed_df = pd.concat([transformed_df, new_df], axis=1)\n",
    "    return transformed_df\n",
    "\n",
    "#automatische ordinal encoder\n",
    "def process_ordinal(df, list_values):\n",
    "    transformed_df = pd.DataFrame()\n",
    "    from sklearn import preprocessing\n",
    "    for item in list_values:\n",
    "        new_df = pd.DataFrame()\n",
    "        enc = preprocessing.OrdinalEncoder()\n",
    "        df[item] = df[item].astype(str)\n",
    "        df[item] = df[item].str.lower()\n",
    "        enc.fit(df[[item]].dropna())\n",
    "        temp = enc.transform(df[[item]].dropna()).shape\n",
    "        new_df[item] = enc.transform(df[[item]].dropna()).flatten()\n",
    "        transformed_df = pd.concat([transformed_df, new_df], axis=1)\n",
    "    return transformed_df\n",
    "\n",
    "data_s = reformat(data_s)\n",
    "data_test_s = reformat(data_test_s)\n",
    "\n",
    "data_t = pd.concat([data_s, data_test_s]).reset_index(drop=True)\n",
    "\n",
    "processed_data_total = process_ordinal(data_t,['screen_surface', 'cpu_type', 'gpu_brand', \"gpu_series\", 'os_type','brand', \"cpu_gen\",\"os\" ])\n",
    "\n",
    "data_t = data_t[['id','pixels_y','ram','ssd','storage','weight', 'discrete_gpu','touchscreen','min_price','max_price']]\n",
    "\n",
    "data_total = pd.concat([processed_data_total, data_t], axis=1).set_index(\"id\")\n",
    "\n",
    "#selecteer de juiste datasets -> training voor model training en test de test dataset van seppe\n",
    "\n",
    "data_total_train = data_total.filter(items=data.id, axis=0)\n",
    "data_total_test = data_total.filter(items=data_test.id, axis=0).iloc[:,0:-2].fillna(0)\n",
    "index=data_total_test.index\n",
    "\n",
    "\n",
    "X_1 = data_total_train.iloc[:,0:-2].fillna(0)\n",
    "y_min = data_total_train.iloc[:,-2].fillna(0)\n",
    "X_2 = data_total_train.iloc[:,0:-2].fillna(0)\n",
    "y_max = data_total_train.iloc[:,-1].fillna(0)\n",
    "\n",
    "pp = preprocessing.PowerTransformer(standardize=True)\n",
    "\n",
    "                            #########################\n",
    "                            ## Build XGBoost model ##\n",
    "\n",
    "    #######################\n",
    "    # MINIMUM price model #\n",
    "    #######################\n",
    "    \n",
    "MinDmatrix= xgb.DMatrix(X_1, label=y_min)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1,y_min,test_size=.3, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate a simple baseline based on mean from the training data\n",
    "mean_train = np.mean(y_train)\n",
    "# Get predictions on the test set\n",
    "baseline_predictions = np.ones(y_test.shape) * mean_train\n",
    "# Compute MAE\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_predictions)\n",
    "print(\"Baseline MAE is {:.2f}\".format(mae_baseline))\n",
    "\n",
    "# Baseline MAE is 448.74\n",
    "\n",
    "# Develop more advanced model with Tuning\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "}\n",
    "\n",
    "params['eval_metric'] = \"mae\"\n",
    "num_boost_round = 999                  #I will apply early stopping later on\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(\"Best MAE: {:.2f} with {} rounds\".format(\n",
    "                 model.best_score,\n",
    "                 model.best_iteration+1))\n",
    "\n",
    "# Best MAE: 176.71 with 12 rounds\n",
    "\n",
    "    # After the 12th tree, adding more rounds did not lead to improvements of MAE on the test dataset. \n",
    "    # This is the MAE of the model with default parameters and an optimal number of boosting rounds, on the test (validation) dataset. \n",
    "    # As you can see, better performance than baseline \n",
    "\n",
    "\n",
    "#In order to tune the other hyperparameters, I use the cv function from XGBoost. \n",
    "#It allows to run cross-validation on the training dataset and returns a mean MAE score.\n",
    "\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'mae'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "cv_results\n",
    "\n",
    "cv_results['test-mae-mean'].min()\n",
    "\n",
    "# Start with max depth and min child weight (maybe should do this in a GridSearch that combines all hyperparms)\n",
    "\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(2,15)\n",
    "    for min_child_weight in range(1,25)\n",
    "]\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best MAE\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n",
    "\n",
    "# Best params: 5, 6, MAE: 152.267633\n",
    "# Did exagerate a bit but best params are max_depth=5 & min_child_weight=6 [still model for min value]\n",
    "\n",
    "params['max_depth'] = 5\n",
    "params['min_child_weight'] = 6\n",
    "\n",
    "# Let's look for subsample and colsample optimal values\n",
    "\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(1,11)]\n",
    "    for colsample in [i/10. for i in range(1,11)]\n",
    "]\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (subsample,colsample)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n",
    "\n",
    "# Best params: 0.8, 0.7, MAE: 145.8399596\n",
    "# best params are subsample=0.8 & colsample= 0.7 [still model for min value]\n",
    "\n",
    "params['subsample'] = .8\n",
    "params['colsample_bytree'] = 0.7\n",
    "\n",
    "# Now i look for the optimal learning rate (eta)\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    # Run and time CV\n",
    "    cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics=['mae'],\n",
    "            early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = eta\n",
    "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))\n",
    "\n",
    "# Best params: 0.01, MAE: 143.9732178\n",
    "# best param for learning rate = 0.01 [still model for min value]\n",
    "\n",
    "params['eta'] = .01\n",
    "\n",
    "# Create param dict with the optimal hyperparams\n",
    "\n",
    "params\n",
    "{'colsample_bytree': 0.7,\n",
    " 'eta': 0.01,\n",
    " 'eval_metric': 'mae',\n",
    " 'max_depth': 5,\n",
    " 'min_child_weight': 6,\n",
    " 'objective': 'reg:linear',\n",
    " 'subsample': 0.8}\n",
    "\n",
    "# Train model\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(\"Best MAE: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1)) \n",
    "\n",
    "# Best MAE: 155.62 in 385 rounds\n",
    "\n",
    "num_boost_round = model.best_iteration + 1\n",
    "\n",
    "# Before using it for predictions, retrain it with the good number of rounds. \n",
    "# Since  the exact best num_boost_round is known, don't need the early_stopping_round anymore.\n",
    "\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")\n",
    "\n",
    "# Make predictions with the model on test data set for MIN PRICE VALUE\n",
    "\n",
    "mean_absolute_error(best_model.predict(dtest), y_test) # MAE = 155.62180246789472 in 385 rounds\n",
    "\n",
    "\n",
    "                                    #########################\n",
    "                                    # Build XGBoost model 2 #\n",
    "                                    #########################\n",
    "    #######################\n",
    "    # MAXIMUM price model #\n",
    "    \n",
    "MaxDmatrix= xgb.DMatrix(X_2, label=y_max)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2,y_max,test_size=.3, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate a simple baseline based on mean from the training data\n",
    "mean_train = np.mean(y_train)\n",
    "# Get predictions on the test set\n",
    "baseline_predictions = np.ones(y_test.shape) * mean_train\n",
    "# Compute MAE\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_predictions)\n",
    "print(\"Baseline MAE MAX is {:.2f}\".format(mae_baseline))\n",
    "\n",
    "# Baseline MAE MAX is 465.21\n",
    "\n",
    "# Develop more advanced model with Tuning\n",
    "\n",
    "params = {\n",
    "    'max_depth':6,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'objective':'reg:linear',\n",
    "}\n",
    "\n",
    "params['eval_metric'] = \"mae\"\n",
    "num_boost_round = 999                  #I will apply early stopping later on\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(\"Best MAE MAX: {:.2f} with {} rounds\".format(\n",
    "                 model.best_score,\n",
    "                 model.best_iteration+1))\n",
    "\n",
    "# Best MAE MAX: 182.83 with 12 rounds\n",
    "# Again, after the 12th tree, adding more rounds did not lead to improvements of MAE on the test dataset. \n",
    "# This is the MAE of the model with default parameters and an optimal number of boosting rounds, on the test (validation) dataset.\n",
    "\n",
    "# Tune other hyperparams, using cv function from XGBoost. Run cross-val on training and return mean MAE score\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'mae'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "cv_results\n",
    "\n",
    "cv_results['test-mae-mean'].min()\n",
    "\n",
    "# Start with max depth and min child weight (maybe should do this in a GridSearch that combines all hyperparms)\n",
    "\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(2,15)\n",
    "    for min_child_weight in range(1,25)\n",
    "]\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best MAE\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n",
    "\n",
    "# MAE 158.5139102 for 21 rounds\n",
    "# Best params: 5, 7, MAE: 152.5074432\n",
    "\n",
    "# Best params are max_depth= 5 & min_child_weight= 7 [still model for Max price]\n",
    "\n",
    "params['max_depth'] = 5\n",
    "params['min_child_weight'] = 7\n",
    "\n",
    "# Let's look for subsample and colsample optimal values\n",
    "\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(1,11)]\n",
    "    for colsample in [i/10. for i in range(1,11)]\n",
    "]\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (subsample,colsample)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n",
    "\n",
    "# MAE 213.44088739999998 for 82 rounds\n",
    "# Best params: 0.9, 1.0, MAE: 151.77083720000002\n",
    "\n",
    "# best params are subsample=0.9 & colsample= 1.0 [still model for MAX value]\n",
    "\n",
    "params['subsample'] = .9\n",
    "params['colsample_bytree'] = 1.0\n",
    "\n",
    "# Now i look for the optimal learning rate (eta)\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    # Run and time CV\n",
    "    cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics=['mae'],\n",
    "            early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = eta\n",
    "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))\n",
    "\n",
    "# Best params: 0.3, MAE: 151.77083720000002\n",
    "# best param for learning rate = 0.3 (really different from min model, 0.01) [still model for MAX value]\n",
    "\n",
    "params['eta'] = .3\n",
    "\n",
    "# Create param dict with the optimal hyperparams\n",
    "\n",
    "params\n",
    "{'colsample_bytree': 1.0,\n",
    " 'eta': 0.3,\n",
    " 'eval_metric': 'mae',\n",
    " 'max_depth': 5,\n",
    " 'min_child_weight': 7,\n",
    " 'objective': 'reg:linear',\n",
    " 'subsample': 0.9}\n",
    "\n",
    "# Train model\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(\"Best MAE MAX: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1)) \n",
    "\n",
    "# Best MAE MAX: 159.17 in 28 rounds (<->Best MAE min was 155.62 in 385! rounds)\n",
    "\n",
    "num_boost_round = model.best_iteration + 1\n",
    "\n",
    "# Before using it for predictions, we should retrain it with the good number of rounds. \n",
    "#Since we know the exact best num_boost_round, we don't need the early_stopping_round anymore.\n",
    "\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")\n",
    "\n",
    "# Make predictions with the model on test data set (VALIDATION SET based on training set)\n",
    "\n",
    "mean_absolute_error(best_model.predict(dtest), y_test) # MAE max= 159.16917813968035 in 28 rounds (MAE Min = 155.62180246789472 in 385 rounds)\n",
    "\n",
    "                                        \n",
    "    ####### Combined result on validation set ######\n",
    "\n",
    "\n",
    "#total MAE based on created VALIDATION set BUT averaged over the PC ID's\n",
    "#the test set was not used\n",
    "\n",
    "MAEmin = 155.62180246789472\n",
    "MAEmax = 159.16917813968035\n",
    "\n",
    "totalerror = MAEmin + MAEmax\n",
    "print(totalerror)\n",
    "\n",
    "# 314.7909806075751"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
